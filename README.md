Case Study: Gender Bias in Machine Learning
---------------------------------------------------------------------------

Machine learning models work by identifying patterns in data and making predictions by attempting to replicate those patterns (Wei, 2020). These patterns are assumed to reflect reality, because they emerge from data concerning actual events and phenomena that have already occurred. In Natural Language Processing (NLP), these patterns can be easily biased (Wei, 2020). Because NLP deals with the link between computer understandings of human language, it is subject to a complex combination of factors including ambiguity, context and nuance (Chowdhury, 2003; Hirschberg and Manning, 2015). It is true that languages follow certain rules (grammar, syntax), however these rules are flexible due to cultural differences, local dialects, colloquialism, etc. Literary and lingual devices like irony and sarcasm further complicate understanding. Finally, implicit cultural bias can result in patterns that cause bias in machine learning predictions. Issues like systemic racial prejudice and gender role bias can and do make their way into machine learning predictions. An example would be the association of ‘black’ to ‘criminal’ and ‘Caucasian’ to ‘police’ (Wei, 2020), or ‘doctor’ to ‘man’ and ‘nurse’ to ‘woman’.

This report will focus on gender bias in natural language processing. Specifically, it will investigate the association of gender to words relating to ‘science’, ‘technology’, ‘engineering’ and ‘mathematics’ – the so-called ‘STEM’ fields. Despite gender equity making great strides globally in the past several decades, there remains a well-documented gap in society when it comes to female participation in STEM careers (ref). Long-held public beliefs regarding gender roles, combined with social expectations associated with these roles has resulted in a lower participation rate of women than men in ‘STEM’ related fields (ref). In recent years, there has been a renewed push, driven by the accessibility of social media, to raise awareness of this gap and encourage women to enter these careers, and to work to identify hurdles to their progression into more senior roles (ref). However, systemic bias towards STEM as particularly ‘male-dominated’ remains…

This investigation will make use of a web crawler to extract data from several websites to form an understanding of gender bias in NLP. Due to its role as a repository of global attitudes and sentiments over time, the crawler will extract data from Twitter, analysing STEM-related hashtags associated with gender in 2010 and again in 2020 to assess any changes in attitude after a decade (and in light of recent Women in STEM campaigns). Secondly, data will be extracted from industry.gov.au regarding STEM participation rates by gender.

Finally, a solution for reducing the gender bias in NLP will be proposed. NLP is an ideal tool for uncovering implicit bias and even understanding the breadth and potentially the source of these biases, and ensuring that these biases do not reinforce harmful prejudices (Sun et al, 2019).

It should be noted that the Tweet dataset is highly imbalanced. There is some risk involved in working with largely imbalanced datasets, since the model could predict all Tweets as male and still be 66% accurate (Kreiger, 2020). For this investigation, I decided against balancing the dataset because we must acknowledge that in the real world, these imbalances do exist.  Balancing the dataset (resampling to reduce relative the number of M and F Tweets) would only serve to artificially eliminate the bias in the first place. The purpose of this investigation is to test a prototype solution to mitigate the gender imbalance in Natural Language processing, so that the bias in prediction models using such data is reduced. In this way, NLP does not perpetuate harmful stereotypes by overclassifying STEM practitioners as male, even though these stereotypes may exist in the real world and in the data. 

442/500

